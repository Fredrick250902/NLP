{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 10-Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\1mscds08\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\1mscds08\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\1mscds08\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"Ram said on monday that WASHINGTON-- In the wake of a string of abuses by Newyork police officers in the 1990s, Loretta E.Lynch, the federal prosecutor in Brooklyn,spoke forcefully about the pain of broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunuication and mistrust fell to law enforcement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ram said on monday that WASHINGTON-- In the wake of a string of abuses by Newyork police officers in the 1990s, Loretta E.Lynch, the federal prosecutor in Brooklyn,spoke forcefully about the pain of broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunuication and mistrust fell to law enforcement'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1mscds08\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Ram/NNP)\n",
      "  said/VBD\n",
      "  on/IN\n",
      "  monday/NN\n",
      "  that/IN\n",
      "  (ORGANIZATION WASHINGTON/NNP)\n",
      "  --/:\n",
      "  In/IN\n",
      "  the/DT\n",
      "  wake/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  string/NN\n",
      "  of/IN\n",
      "  abuses/NNS\n",
      "  by/IN\n",
      "  (ORGANIZATION Newyork/NNP)\n",
      "  police/NNS\n",
      "  officers/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  1990s/CD\n",
      "  ,/,\n",
      "  (PERSON Loretta/NNP E.Lynch/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  federal/JJ\n",
      "  prosecutor/NN\n",
      "  in/IN\n",
      "  (GPE Brooklyn/NNP)\n",
      "  ,/,\n",
      "  spoke/VBD\n",
      "  forcefully/RB\n",
      "  about/IN\n",
      "  the/DT\n",
      "  pain/NN\n",
      "  of/IN\n",
      "  broken/JJ\n",
      "  trust/NN\n",
      "  that/IN\n",
      "  African-Americans/NNP\n",
      "  felt/VBD\n",
      "  and/CC\n",
      "  said/VBD\n",
      "  the/DT\n",
      "  responsibility/NN\n",
      "  for/IN\n",
      "  repairing/VBG\n",
      "  generations/NNS\n",
      "  of/IN\n",
      "  miscommunuication/NN\n",
      "  and/CC\n",
      "  mistrust/NN\n",
      "  fell/VBD\n",
      "  to/TO\n",
      "  law/NN\n",
      "  enforcement/NN)\n"
     ]
    }
   ],
   "source": [
    "tokens=wt(sentence1)\n",
    "tags=pos_tag(tokens)\n",
    "ne_tree=ne_chunk(tags)\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tree=ne_chunk(pos_tag(wt(sentence1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Ram/NNP)\n",
      "('said', 'VBD')\n",
      "('on', 'IN')\n",
      "('monday', 'NN')\n",
      "('that', 'IN')\n",
      "(ORGANIZATION WASHINGTON/NNP)\n",
      "('--', ':')\n",
      "('In', 'IN')\n",
      "('the', 'DT')\n",
      "('wake', 'NN')\n",
      "('of', 'IN')\n",
      "('a', 'DT')\n",
      "('string', 'NN')\n",
      "('of', 'IN')\n",
      "('abuses', 'NNS')\n",
      "('by', 'IN')\n",
      "(ORGANIZATION Newyork/NNP)\n",
      "('police', 'NNS')\n",
      "('officers', 'NNS')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('1990s', 'CD')\n",
      "(',', ',')\n",
      "(PERSON Loretta/NNP E.Lynch/NNP)\n",
      "(',', ',')\n",
      "('the', 'DT')\n",
      "('federal', 'JJ')\n",
      "('prosecutor', 'NN')\n",
      "('in', 'IN')\n",
      "(GPE Brooklyn/NNP)\n",
      "(',', ',')\n",
      "('spoke', 'VBD')\n",
      "('forcefully', 'RB')\n",
      "('about', 'IN')\n",
      "('the', 'DT')\n",
      "('pain', 'NN')\n",
      "('of', 'IN')\n",
      "('broken', 'JJ')\n",
      "('trust', 'NN')\n",
      "('that', 'IN')\n",
      "('African-Americans', 'NNP')\n",
      "('felt', 'VBD')\n",
      "('and', 'CC')\n",
      "('said', 'VBD')\n",
      "('the', 'DT')\n",
      "('responsibility', 'NN')\n",
      "('for', 'IN')\n",
      "('repairing', 'VBG')\n",
      "('generations', 'NNS')\n",
      "('of', 'IN')\n",
      "('miscommunuication', 'NN')\n",
      "('and', 'CC')\n",
      "('mistrust', 'NN')\n",
      "('fell', 'VBD')\n",
      "('to', 'TO')\n",
      "('law', 'NN')\n",
      "('enforcement', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for i in  ne_tree:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Count and print the number of PERSON,LOCATION and ORGANIZATION in the given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'Ram': 1, 'NNP': 1})]\n",
      "[Counter({'WASHINGTON': 1, 'NNP': 1})]\n",
      "[Counter({'Newyork': 1, 'NNP': 1})]\n",
      "[Counter({'Loretta': 1, 'NNP': 1}), Counter({'E.Lynch': 1, 'NNP': 1})]\n",
      "[Counter({'Brooklyn': 1, 'NNP': 1})]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence1))):\n",
    "    if hasattr(chunk,\"label\"):\n",
    "        print([Counter(label)for label in chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Does named entity,\"police officers\" grt recognized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'WASHINGTON', 'Newyork', 'Loretta E.Lynch', 'Brooklyn']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence1)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<NN><NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'write a regular expression patter to detect this.you will need nltk.regexpparser class to define pattern and parse terms to detect patterns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'WASHINGTON', 'the wake', 'a string', 'Newyork', 'Loretta E.Lynch', 'the federal prosecutor', 'Brooklyn', 'the pain', 'the responsibility']\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Does the named entity ,'The Top Federal Prosecutor' get recognized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ram', 'NNP'), ('said', 'VBD'), ('on', 'IN'), ('monday', 'NN'), ('that', 'IN'), ('WASHINGTON', 'NNP'), ('--', ':'), ('In', 'IN'), Tree('NP', [('the', 'DT'), ('wake', 'NN')]), ('of', 'IN'), Tree('NP', [('a', 'DT'), ('string', 'NN')]), ('of', 'IN'), ('abuses', 'NNS'), ('by', 'IN'), ('Newyork', 'NNP'), ('police', 'NNS'), ('officers', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('1990s', 'CD'), (',', ','), ('Loretta', 'NNP'), ('E.Lynch', 'NNP'), (',', ','), Tree('NP', [('the', 'DT'), ('federal', 'JJ'), ('prosecutor', 'NN')]), ('in', 'IN'), ('Brooklyn', 'NNP'), (',', ','), ('spoke', 'VBD'), ('forcefully', 'RB'), ('about', 'IN'), Tree('NP', [('the', 'DT'), ('pain', 'NN')]), ('of', 'IN'), ('broken', 'JJ'), ('trust', 'NN'), ('that', 'IN'), ('African-Americans', 'NNP'), ('felt', 'VBD'), ('and', 'CC'), ('said', 'VBD'), Tree('NP', [('the', 'DT'), ('responsibility', 'NN')]), ('for', 'IN'), ('repairing', 'VBG'), ('generations', 'NNS'), ('of', 'IN'), ('miscommunuication', 'NN'), ('and', 'CC'), ('mistrust', 'NN'), ('fell', 'VBD'), ('to', 'TO'), ('law', 'NN'), ('enforcement', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "parse = cp.parse(tags)\n",
    "print(parse[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write a regular expression pattern to detect this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'WASHINGTON', 'the wake', 'a string', 'Newyork', 'Loretta E.Lynch', 'Brooklyn', 'the pain', 'the responsibility']\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT><JACJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract All Named Entities From The Following Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Observe The Output. Does Your Code Recognize The NE Showns in Bold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('GPE', [('European', 'JJ')]), ('authorities', 'NNS'), ('fined', 'VBD'), Tree('PERSON', [('Google', 'NNP')]), ('a', 'DT'), ('record', 'NN'), ('$', '$'), ('5.1', 'CD'), ('billion', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('its', 'PRP$'), ('power', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mobile', 'JJ'), ('phone', 'NN'), ('market', 'NN'), ('and', 'CC'), ('ordered', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('to', 'TO'), ('alter', 'VB'), ('its', 'PRP$'), ('practices', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "token=wt(sentence2)\n",
    "tag=nltk.pos_tag(token)\n",
    "ne_tree=ne_chunk(tag)\n",
    "print(ne_tree[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'write a regular expression that recognizes the entity,$ 5.1 billion detect andprint this'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'Google', '5.1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence2)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<CD>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QUESTION 2:'write a regular expression that recognizes the entity,\"the mobile phone\" and similar to this entity such as \"the company\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'Google', 'a record', 'the mobile phone', 'the company']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence2)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BEEF TENDERLOIN STEAKS WITH SMOKY BACON-BORBON SAUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipe.txt\", \"w\") as file:\n",
    "    file.write(\"1 1/2 cups dry red wine\\n\")\n",
    "    file.write(\"3 cloves garlic\\n\")\n",
    "    file.write(\"1 3/4 cups beef broth\\n\")\n",
    "    file.write(\"1 1/4 cups chicken broth\\n\")\n",
    "    file.write(\"1 1/2 tablespoons tomato paste\\n\")\n",
    "    file.write(\"1 bay leaf\\n\")\n",
    "    file.write(\"1 sprig thyme\\n\")\n",
    "    file.write(\"8 ounces bacon cut into 1/4 inch pieces\\n\")\n",
    "    file.write(\"1 tablespoon flour\\n\")\n",
    "    file.write(\"1 tablespoon butter\\n\")\n",
    "    file.write(\"4 1 inch rib-eye steaks\\n\")\n",
    "    file.write(\"1 tablespoon bourbon whiskey\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1/2 cups dry red wine\n",
      "3 cloves garlic\n",
      "1 3/4 cups beef broth\n",
      "1 1/4 cups chicken broth\n",
      "1 1/2 tablespoons tomato paste\n",
      "1 bay leaf\n",
      "1 sprig thyme\n",
      "8 ounces bacon cut into 1/4 inch pieces\n",
      "1 tablespoon flour\n",
      "1 tablespoon butter\n",
      "4 1 inch rib-eye steaks\n",
      "1 tablespoon bourbon whiskey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f=open(\"recipe.txt\", \"r\")\n",
    "text=f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m tokens\u001b[38;5;241m=\u001b[39mwt(text)\n\u001b[1;32m----> 2\u001b[0m tags\u001b[38;5;241m=\u001b[39m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ne_tree\u001b[38;5;241m=\u001b[39mne_chunk(tags)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(ne_tree[:])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "tokens=wt(text)\n",
    "tags=pos_tag(tokens)\n",
    "ne_tree=ne_chunk(tags)\n",
    "print(ne_tree[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tree=ne_chunk(pos_tag(wt(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  ne_tree:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))):\n",
    "    if hasattr(chunk,\"label\"):\n",
    "        print([Counter(label)for label in chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
